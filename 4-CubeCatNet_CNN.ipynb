{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ed05bb-fe72-45c8-aaf4-f067ee02838e",
   "metadata": {},
   "source": [
    "# Notebook 4: Classification using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38388419-5a1e-458f-9a07-1179014901ba",
   "metadata": {},
   "source": [
    "**Only run it if you’re adopting or experimenting with this model, as it can take up to two hours to train.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0cbf42-add7-486e-9e2d-6f04051b812e",
   "metadata": {},
   "source": [
    "In this notebook, we will use a CNN model to classify the images, following the approach used in the following [paper](https://ui.adsabs.harvard.edu/abs/2023SPIE12729E..0KC/abstract)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13095d08-82fa-49dc-8613-e8d4d1320555",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f39eb1-6f50-40c3-8468-226c0c8ee6b4",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f576c9-784f-49d2-bf5d-6978edeb89d4",
   "metadata": {},
   "source": [
    "First, we’ll load the saved image and label data from the NumPy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4a8dd1-52ed-4514-8cd1-a59980fab45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from NumPy files.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Importing NumPy for numerical operations and array handling\n",
    "\n",
    "# Load the images and labels back from the saved NumPy files\n",
    "train_images = np.load('train_images.npy')  # Load image training data\n",
    "train_labels = np.load('train_labels.npy')  # Load label training data\n",
    "\n",
    "print(\"Data loaded successfully from NumPy files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953fc9a-d986-47e1-8872-213f8518cdb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a46294-7dfd-46c0-9146-d53220c58dbb",
   "metadata": {},
   "source": [
    "### Train CubeCatNet CNN mdoel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c005cb-a745-4594-8aab-15b67e0fcdac",
   "metadata": {},
   "source": [
    "We will define and train a Convolutional Neural Network (CNN) model that was defined in [link](https://ui.adsabs.harvard.edu/abs/2023SPIE12729E..0KC/abstract)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54646112-a70d-4764-93bc-7f0b44e267b1",
   "metadata": {},
   "source": [
    "**⚠️**: When running this model, or any other deep learning model, make sure it’s not being run in parallel with your other team members. Running multiple models simultaneously on shared resources like CPUs can severely impact performance, leading to slower training times and potential resource conflicts. Coordinate with your team to ensure efficient use of the available hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7dbc5a-b183-4974-a3fd-7ce74c4fa66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 20:32:57.406903: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-15 20:32:57.413271: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-15 20:32:57.432603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-15 20:32:57.456901: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-15 20:32:57.464174: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 20:32:57.484454: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 20:32:58.610337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/idia/projects/hack4dev/CubeSat_ImageClassify/vCube/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined and compiled successfully.\n",
      "Epoch 1/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 3s/step - accuracy: 0.6455 - loss: 0.9986\n",
      "Epoch 2/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 3s/step - accuracy: 0.9907 - loss: 0.0513\n",
      "Epoch 3/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 3s/step - accuracy: 0.9953 - loss: 0.0176\n",
      "Epoch 4/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 3s/step - accuracy: 0.9970 - loss: 0.0136\n",
      "Epoch 5/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 3s/step - accuracy: 0.9959 - loss: 0.0134\n",
      "Epoch 6/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 3s/step - accuracy: 0.9642 - loss: 0.1154\n",
      "Epoch 8/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m692s\u001b[0m 3s/step - accuracy: 0.9993 - loss: 0.0032\n",
      "Epoch 9/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m694s\u001b[0m 3s/step - accuracy: 0.9989 - loss: 0.0033\n",
      "Epoch 10/10\n",
      "\u001b[1m203/203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m694s\u001b[0m 3s/step - accuracy: 0.9991 - loss: 0.0027\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # Importing Sequential to build the model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense  # Importing necessary layers for the CNN\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode the labels (assuming you have 5 classes)\n",
    "train_labels = to_categorical(train_labels, num_classes=5)\n",
    "\n",
    "# Define the CNN model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(512, 512, 3)),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    Conv2D(32, (3, 3), activation='relu'),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    Conv2D(64, (3, 3), activation='relu'),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    Conv2D(128, (3, 3), activation='relu'),  # Convolutional layer + ReLU activation\n",
    "    MaxPooling2D((2, 2)),  # Max pooling layer\n",
    "    GlobalAveragePooling2D(),  # Global average pooling layer\n",
    "    Dense(5, activation='softmax')  # Output layer with 5 neurons (one for each class) + Softmax activation\n",
    "])\n",
    "\n",
    "# Compile the model with appropriate loss function, optimizer, and metrics\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model defined and compiled successfully.\")\n",
    "\n",
    "# Train the model on the training data\n",
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=10,  # Number of epochs\n",
    "    batch_size=64,  # Batch size\n",
    ")\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5cfc3-f2cd-4918-a85f-b3f1f7a9e505",
   "metadata": {},
   "source": [
    "##### **⚠️ Freeing up Space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec17f896-bbef-4caa-9b56-ba9d228dfaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images, and train_labels removed from memory.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Since we will no longer need the original training data (train_images), we can remove it from memory\n",
    "del train_images, train_labels\n",
    "\n",
    "# Force garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "print(\"train_images, and train_labels removed from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01be5fd-f911-410b-81e4-f0cd21f4cb4f",
   "metadata": {},
   "source": [
    "#### Saving the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c650f7-cf36-4108-ad8b-b6cb7fbff267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('cnn_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9d935-fda3-4ea3-8b33-4d5bd7cefc3a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e81814-27e8-4dbd-b73e-5976943b569a",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d4b0095-2b77-4135-88df-9005de540339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us first load the testing data\n",
    "test_images = np.load('test_images.npy')      # Load image test data\n",
    "test_labels = np.load('test_labels.npy')      # Load label test data\n",
    "test_labels = to_categorical(test_labels, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d4ddbce-5789-4d82-88d7-5578e891a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 321ms/step\n",
      "(3237, 5)\n",
      "(3237, 5)\n",
      "Evaluation Metrics:\n",
      "evaluation_time: 36.03 seconds\n",
      "peak_memory_usage: 9925.21 MB\n",
      "average_cpu_usage: 1032.96%\n",
      "accuracy: 0.9966\n",
      "f1_score: 0.9966\n",
      "pipeline_size: 1.16 MB\n"
     ]
    }
   ],
   "source": [
    "from source.pre import evaluate_pipeline\n",
    "\n",
    "def preprocessing_fn(X):\n",
    "    \n",
    "    return X\n",
    "    \n",
    "# Evaluate the pipeline\n",
    "metrics = evaluate_pipeline(model, test_images, test_labels, preprocessing_fn)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if key == 'evaluation_time':\n",
    "        print(f\"{key}: {value:.2f} seconds\")\n",
    "    elif key == 'pipeline_size':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'peak_memory_usage':\n",
    "        print(f\"{key}: {value:.2f} MB\")\n",
    "    elif key == 'average_cpu_usage':\n",
    "        print(f\"{key}: {value:.2f}%\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc867bfa-7257-4523-a593-6ee21c8a4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now try delete the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406d3d2-e4f3-47b9-a117-b18a1b48e808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CubeHackKer",
   "language": "python",
   "name": "cubehackker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
